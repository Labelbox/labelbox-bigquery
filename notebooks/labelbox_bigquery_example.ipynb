{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9SOR4pplArv3",
        "cpq--1Yb8Nrd",
        "qCoxeWHDgR8z",
        "-2LzsXic898t",
        "BYtdFPTz9Weq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "kKeJRAX-nDt8"
      },
      "outputs": [],
      "source": [
        "!pip install \"labelbox[data]\" --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labelbox-BigQuery `client.py` copy/pasted"
      ],
      "metadata": {
        "id": "9SOR4pplArv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import labelbox\n",
        "from labelbox import Client as labelboxClient\n",
        "from labelbox.schema.data_row_metadata import DataRowMetadataKind\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "\n",
        "class Client:\n",
        "    \"\"\" A LabelBigQuery Client, containing a Labelbox Client and BigQuery Client Object\n",
        "    Args:\n",
        "        lb_api_key                  :   Required (str) - Labelbox API Key\n",
        "        google_key                  :   Required (dict) - Google Service Account Permissions dict, how to create one here: https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating\n",
        "        google_project_name         :   Required (str) - Google Project ID / Name\n",
        "        lb_endpoint                 :   Optinoal (bool) - Labelbox GraphQL endpoint\n",
        "        lb_enable_experimental      :   Optional (bool) - If `True` enables experimental Labelbox SDK features\n",
        "        lb_app_url                  :   Optional (str) - Labelbox web app URL\n",
        "    Attributes:\n",
        "        lb_client                   :   labelbox.Client object\n",
        "        bq_client                   :   bigquery.Client object\n",
        "    Key Functions:\n",
        "        create_data_rows_from_table :   Creates Labelbox data rows (and metadata) given a BigQuery table\n",
        "        create_table_from_dataset   :   Creates a BigQuery table given a Labelbox dataset\n",
        "        upsert_table_metadata       :   Updates BigQuery table metadata columns given a Labelbox dataset\n",
        "        upsert_labelbox_metadata    :   Updates Labelbox metadata given a BigQuery table\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        lb_api_key=None, \n",
        "        google_project_name=None,\n",
        "        google_key=None,\n",
        "        lb_endpoint='https://api.labelbox.com/graphql', \n",
        "        lb_enable_experimental=False, \n",
        "        lb_app_url=\"https://app.labelbox.com\"):  \n",
        "\n",
        "        self.lb_client = labelboxClient(lb_api_key, endpoint=lb_endpoint, enable_experimental=lb_enable_experimental, app_url=lb_app_url)\n",
        "        bq_creds = service_account.Credentials.from_service_account_file(google_key)\n",
        "        self.bq_client = bigquery.Client(project=google_project_name, credentials=bq_creds)\n",
        "    \n",
        "    def _sync_metadata_fields(self, bq_table_id, metadata_index={}):\n",
        "        \"\"\" Ensures Labelbox's Metadata Ontology has all necessary metadata fields given a metadata_index\n",
        "        Args:\n",
        "            bq_table_id     :   Required (str) - Table ID structured in the following schema: google_project_name.dataset_name.table_name\n",
        "            metadata_index  :   Optional (dict) - Dictionary where {key=column_name : value=metadata_type} - metadata_type must be one of \"enum\", \"string\", \"datetime\" or \"number\"\n",
        "        Returns:\n",
        "            True if the sync is successful, False if not\n",
        "        \"\"\"\n",
        "        # Get your metadata ontology\n",
        "        lb_mdo = self.lb_client.get_data_row_metadata_ontology()\n",
        "        bq_table = self.bq_client.get_table(bq_table_id)\n",
        "        # Convert your meatdata_index values from strings into labelbox.schema.data_row_metadata.DataRowMetadataKind types\n",
        "        conversion = {\"enum\" : DataRowMetadataKind.enum, \"string\" : DataRowMetadataKind.string, \"datetime\" : DataRowMetadataKind.datetime, \"number\" : DataRowMetadataKind.number}\n",
        "        # Grab all the metadata field names\n",
        "        lb_metadata_names = [field['name'] for field in lb_mdo._get_ontology()]\n",
        "        # Iterate over your metadata_index, if a metadata_index key is not an existing metadata_field, then create it in Labelbox\n",
        "        if metadata_index:\n",
        "            for column_name in metadata_index.keys():\n",
        "                metadata_type = metadata_index[column_name]\n",
        "                if metadata_type not in conversion.keys():\n",
        "                    print(f'Error: Invalid value for metadata_index field {column_name}: {metadata_type}')\n",
        "                    return False\n",
        "                if column_name not in lb_metadata_names:\n",
        "                    # For enum fields, grab all the unique values from that column as a list\n",
        "                    if metadata_type == \"enum\":\n",
        "                        query_job = self.bq_client.query(f\"\"\"SELECT DISTINCT {column_name} FROM {bq_table.table_id}\"\"\")\n",
        "                        enum_options = [row.values()[0] for row in query_job]\n",
        "                    else:\n",
        "                        enum_options = []\n",
        "                    lb_mdo.create_schema(name=column_name, kind=conversion[metadata_type], options=enum_options)\n",
        "                    lb_mdo = self.lb_client.get_data_row_metadata_ontology()\n",
        "                    lb_metadata_names = [field['name'] for field in lb_mdo._get_ontology()]\n",
        "        # Iterate over your metadata_index, if a metadata_index key is not an existing column name, then create it in BigQuery  \n",
        "        if metadata_index:\n",
        "            column_names = [schema_field.name for schema_field in bq_table.schema]\n",
        "            for metadata_field_name in metadata_index.keys():\n",
        "                if metadata_field_name not in column_names:\n",
        "                    original_schema = bq_table.schema\n",
        "                    new_schema = original_schema[:]\n",
        "                    new_schema.append(bigquery.SchemaField(metadata_field_name, \"STRING\"))\n",
        "                    bq_table.schema = new_schema\n",
        "                    bq_table = self.bq_client.update_table(bq_table, [\"schema\"])                   \n",
        "        # Track data rows loaded from BigQuery\n",
        "        if \"lb_integration_source\" not in lb_metadata_names:\n",
        "            lb_mdo.create_schema(name=\"lb_integration_source\", kind=DataRowMetadataKind.string)\n",
        "        return True\n",
        "\n",
        "    def __get_metadata_schema_to_name_key(self, lb_mdo:labelbox.schema.data_row_metadata.DataRowMetadataOntology, divider=\"///\", invert=False):\n",
        "        \"\"\" Creates a dictionary where {key=metadata_schema_id: value=metadata_name_key} \n",
        "        - name_key is name for all metadata fields, and for enum options, it is \"parent_name{divider}child_name\"\n",
        "        Args:\n",
        "            lb_mdo              :   Required (labelbox.schema.data_row_metadata.DataRowMetadataOntology) - Labelbox metadata ontology\n",
        "            divider             :   Optional (str) - String separating parent and enum option metadata values\n",
        "            invert              :   Optional (bool) - If True, will make the name_key the dictionary key and the schema_id the dictionary value\n",
        "        Returns:\n",
        "            Dictionary where {key=metadata_schema_id: value=metadata_name_key}\n",
        "        \"\"\"\n",
        "        lb_metadata_dict = lb_mdo.reserved_by_name\n",
        "        lb_metadata_dict.update(lb_mdo.custom_by_name)\n",
        "        metadata_schema_to_name_key = {}\n",
        "        for metadata_field_name in lb_metadata_dict:\n",
        "            if type(lb_metadata_dict[metadata_field_name]) == dict:\n",
        "                metadata_schema_to_name_key[lb_metadata_dict[metadata_field_name][next(iter(lb_metadata_dict[metadata_field_name]))].parent] = str(metadata_field_name)\n",
        "                for enum_option in lb_metadata_dict[metadata_field_name]:\n",
        "                    metadata_schema_to_name_key[lb_metadata_dict[metadata_field_name][enum_option].uid] = f\"{str(metadata_field_name)}{str(divider)}{str(enum_option)}\"\n",
        "            else:\n",
        "                metadata_schema_to_name_key[lb_metadata_dict[metadata_field_name].uid] = str(metadata_field_name)\n",
        "        return_value = metadata_schema_to_name_key if not invert else {v:k for k,v in metadata_schema_to_name_key.items()}\n",
        "        return return_value\n",
        "    \n",
        "    def __batch_create_data_rows(self, client, dataset, global_key_to_upload_dict, skip_duplicates=True, batch_size=20000):\n",
        "        \"\"\" Checks to make sure no duplicate global keys are uploaded before batch uploading data rows\n",
        "        Args:\n",
        "            client                      : Required (labelbox.client.Client) : Labelbox Client object\n",
        "            dataset                     : Required (labelbox.dataset.Dataset) : Labelbox Dataset object\n",
        "            global_key_to_upload_dict   : Required (dict) : Dictionary where {key=global_key : value=data_row_dict to-be-uploaded to Labelbox}\n",
        "            skip_duplicates             : Optional (bool) - If True, will skip duplicate global_keys, otherwise will generate a unique global_key with a suffix \"_1\", \"_2\" and so on\n",
        "            batch_size                  : Optional (int) : Upload batch size, 20,000 is recommended\n",
        "        Returns:\n",
        "            A concatenated list of upload results for all batch uploads\n",
        "        \"\"\"\n",
        "        def __check_global_keys(client, global_keys):\n",
        "            \"\"\" Checks if data rows exist for a set of global keys\n",
        "            Args:\n",
        "                client                  : Required (labelbox.client.Client) : Labelbox Client object\n",
        "                global_keys             : Required (list(str)) : List of global key strings\n",
        "            Returns:\n",
        "                True if global keys are available, False if not\n",
        "            \"\"\"\n",
        "            # Create a query job to get data row IDs given global keys\n",
        "            query_str_1 = \"\"\"query get_datarow_with_global_key($global_keys:[ID!]!){dataRowsForGlobalKeys(where:{ids:$global_keys}){jobId}}\"\"\"\n",
        "            query_job_id = client.execute(query_str_1, {\"global_keys\":global_keys})['dataRowsForGlobalKeys']['jobId']\n",
        "            # Get the results of this query job\n",
        "            query_str_2 = \"\"\"query get_job_result($job_id:ID!){dataRowsForGlobalKeysResult(jobId:{id:$job_id}){data{\n",
        "                            accessDeniedGlobalKeys\\ndeletedDataRowGlobalKeys\\nfetchedDataRows{id}\\nnotFoundGlobalKeys}jobStatus}}\"\"\"\n",
        "            res = client.execute(query_str_2, {\"job_id\":query_job_id})['dataRowsForGlobalKeysResult']['data']\n",
        "            return res    \n",
        "        global_keys_list = list(global_key_to_upload_dict.keys())\n",
        "        payload = __check_global_keys(client, global_keys_list)\n",
        "        loop_counter = 0\n",
        "        while len(payload['notFoundGlobalKeys']) != len(global_keys_list):\n",
        "            loop_counter += 1\n",
        "            if payload['deletedDataRowGlobalKeys']:\n",
        "                client.clear_global_keys(payload['deletedDataRowGlobalKeys'])\n",
        "                payload = __check_global_keys(client, global_keys_list)\n",
        "                continue\n",
        "            if payload['fetchedDataRows']:\n",
        "                for i in range(0, len(payload['fetchedDataRows'])):\n",
        "                    if payload['fetchedDataRows'][i] != \"\":\n",
        "                        if skip_duplicates:\n",
        "                            global_key = str(global_keys_list[i])\n",
        "                            del global_key_to_upload_dict[str(global_key)]\n",
        "                        else:\n",
        "                            global_key = str(global_keys_list[i])\n",
        "                            new_upload_dict = global_key_to_upload_dict[str(global_key)]\n",
        "                            del global_key_to_upload_dict[str(global_key)]\n",
        "                            new_global_key = f\"{global_key}_{loop_counter}\"\n",
        "                            new_upload_dict['global_key'] = new_global_key\n",
        "                            global_key_to_upload_dict[new_global_key] = new_upload_dict\n",
        "                global_keys_list = list(global_key_to_upload_dict.keys())\n",
        "                payload = __check_global_keys(client, global_keys_list)\n",
        "        upload_list = list(global_key_to_upload_dict.values())\n",
        "        upload_results = []\n",
        "        for i in range(0,len(upload_list),batch_size):\n",
        "            batch = upload_list[i:] if i + batch_size >= len(upload_list) else upload_list[i:i+batch_size]\n",
        "            task = lb_dataset.create_data_rows(batch)\n",
        "            errors = task.errors\n",
        "            if errors:\n",
        "                print(f'Data Row Creation Error: {errors}')\n",
        "                return errors\n",
        "            else:\n",
        "                upload_results.extend(task.result)\n",
        "        return upload_results   \n",
        "\n",
        "    def create_data_rows_from_table(self, bq_table_id, lb_dataset, row_data_col, global_key_col=None, external_id_col=None, metadata_index={}, attachment_index={}, skip_duplicates=False):\n",
        "        \"\"\" Creates Labelbox data rows given a BigQuery table and a Labelbox Dataset\n",
        "        Args:\n",
        "            bq_table_id       : Required (str) - BigQuery Table ID structured in the following format: \"google_project_name.dataset_name.table_name\"\n",
        "            lb_dataset        : Required (labelbox.schema.dataset.Dataset) - Labelbox dataset to add data rows to            \n",
        "            row_data_col      : Required (str) - Column name where the data row row data URL is located\n",
        "            global_key_col    : Optional (str) - Column name where the data row global key is located - defaults to the row_data column\n",
        "            external_id_col   : Optional (str) - Column name where the data row external ID is located - defaults to the row_data column\n",
        "            metadata_index    : Optional (dict) - Dictionary where {key=column_name : value=metadata_type} - metadata_type must be one of \"enum\", \"string\", \"datetime\" or \"number\"\n",
        "            attachment_index  : Optional (dict) - Dictionary where {key=column_name : value=attachment_type} - attachment_type must be one of \"IMAGE\", \"VIDEO\", \"TEXT\", \"HTML\"\n",
        "            skip_duplicates   : Optional (bool) - If True, will skip duplicate global_keys, otherwise will generate a unique global_key with a suffix \"_1\", \"_2\" and so on\n",
        "        Returns:\n",
        "            List of errors from data row upload - if successful, is an empty list\n",
        "        \"\"\"\n",
        "        # Sync metadata index keys with metadata ontology\n",
        "        check = self._sync_metadata_fields(bq_table_id, metadata_index)\n",
        "        if not check:\n",
        "          return None\n",
        "        # Create a metadata_schema_dict where {key=metadata_field_name : value=metadata_schema_id}\n",
        "        lb_mdo = self.lb_client.get_data_row_metadata_ontology()\n",
        "        metadata_name_key_to_schema = self.__get_metadata_schema_to_name_key(lb_mdo, invert=True)\n",
        "        # Ensure your row_data, external_id, global_key and metadata_index keys are in your BigQery table, build your query\n",
        "        bq_table = self.bq_client.get_table(bq_table_id)\n",
        "        column_names = [schema_field.name for schema_field in bq_table.schema]\n",
        "        if row_data_col not in column_names:\n",
        "            print(f'Error: No column matching provided \"row_data_col\" column value {row_data_col}')\n",
        "            return None\n",
        "        else:\n",
        "            index_value = 0\n",
        "            query_lookup = {row_data_col:index_value}\n",
        "            col_query = row_data_col\n",
        "            index_value += 1\n",
        "        if global_key_col:\n",
        "            if global_key_col not in column_names:\n",
        "                 print(f'Error: No column matching provided \"global_key_col\" column value {global_key_col}')\n",
        "                 return None\n",
        "            else:\n",
        "                col_query += f\", {global_key_col}\"\n",
        "                query_lookup[global_key_col] = index_value\n",
        "                index_value += 1\n",
        "        else:\n",
        "            print(f'No global_key_col provided, will default global_key_col to {row_data_col} column')\n",
        "            global_key_col = row_data_col\n",
        "            col_query += f\", {global_key_col}\"\n",
        "            query_lookup[global_key_col] = index_value\n",
        "            index_value += 1\n",
        "        if external_id_col:\n",
        "            if external_id_col not in column_names:\n",
        "                print(f'Error: No column matching provided \"gloabl_key\" column value {external_id_col}')\n",
        "                return None\n",
        "            else:\n",
        "                col_query+= f\", {external_id_col}\"    \n",
        "                query_lookup[external_id_col] = index_value            \n",
        "                index_value += 1    \n",
        "        if metadata_index:\n",
        "            for metadata_field_name in metadata_index:\n",
        "                mdf = metadata_field_name.replace(\" \", \"_\")\n",
        "                if mdf not in column_names:\n",
        "                    print(f'Error: No column matching metadata_index key {metadata_field_name}')\n",
        "                    return None\n",
        "                else:\n",
        "                    col_query+=f', {mdf}'\n",
        "                    query_lookup[mdf] = index_value\n",
        "                    index_value += 1\n",
        "        if attachment_index:\n",
        "            for attachment_field_name in attachment_index:\n",
        "                atf = attachment_field_name.replace(\" \", \"_\")\n",
        "                attachment_whitelist = [\"IMAGE\", \"VIDEO\", \"TEXT\", \"HTML\"]\n",
        "                if attachment_index[attachment_field_name] not in attachment_whitelist:\n",
        "                    print(f'Error: Invalid value for attachment_index key {attachment_field_name} : {attachment_index[attachment_field_name]}\\n must be one of {attachment_whitelist}')\n",
        "                    return None\n",
        "                if atf not in column_names:\n",
        "                    print(f'Error: No column matching attachment_index key {attachment_field_name}')\n",
        "                    return None\n",
        "                else:\n",
        "                    col_query+=f', {atf}'\n",
        "                    query_lookup[atf] = index_value\n",
        "                    index_value += 1                \n",
        "        # Query your row_data, external_id, global_key and metadata_index key columns from \n",
        "        query = f\"\"\"SELECT {col_query} FROM {bq_table.project}.{bq_table.dataset_id}.{bq_table.table_id}\"\"\"\n",
        "        query_job = self.bq_client.query(query)\n",
        "        # Iterate over your query payload to construct a list of data row dictionaries in Labelbox format\n",
        "        global_key_to_upload_dict = {}\n",
        "        for row in query_job:\n",
        "            data_row_upload_dict = {\n",
        "                \"row_data\" : row[query_lookup[row_data_col]],\n",
        "                \"metadata_fields\" : [{\"schema_id\":metadata_name_key_to_schema['lb_integration_source'],\"value\":\"BigQuery\"}],\n",
        "                \"global_key\" : row[query_lookup[global_key_col]]\n",
        "            }\n",
        "            if external_id_col:\n",
        "                data_row_upload_dict['external_id'] = row[query_lookup[external_id_col]]\n",
        "            if metadata_index:\n",
        "                for metadata_field_name in metadata_index:\n",
        "                    mdf = metadata_field_name.replace(\" \", \"_\")\n",
        "                    metadata_schema_id = metadata_name_key_to_schema[metadata_field_name]\n",
        "                    mdx_value = f\"{metadata_field_name}///{row[query_lookup[mdf]]}\"\n",
        "                    if mdx_value in metadata_name_key_to_schema.keys():\n",
        "                        metadata_value = metadata_name_key_to_schema[mdx_value]\n",
        "                    else:\n",
        "                        metadata_value = row[query_lookup[mdf]]\n",
        "                    data_row_upload_dict['metadata_fields'].append({\n",
        "                        \"schema_id\" : metadata_schema_id,\n",
        "                        \"value\" : metadata_value\n",
        "                    })\n",
        "            if attachment_index:\n",
        "                data_row_upload_dict['attachments'] = [{\"type\" : attachment_index[attachment_field_name], \"value\" : row[query_lookup[attachment_field_name]]} for attachment_field_name in attachment_index]\n",
        "            global_key_to_upload_dict[row[query_lookup[global_key_col]]] = data_row_upload_dict\n",
        "        # Batch upload your list of data row dictionaries in Labelbox format\n",
        "        upload_results = self.__batch_create_data_rows(client=self.lb_client, dataset=lb_dataset, global_key_to_upload_dict=global_key_to_upload_dict)\n",
        "        print(f'Success')\n",
        "        return upload_results\n",
        "\n",
        "    def create_table_from_dataset(self, bq_dataset_id, bq_table_name, lb_dataset, metadata_index={}):\n",
        "        \"\"\" Creates a BigQuery Table from a Labelbox dataset given a BigQuery Dataset ID, desired Table name, and optional metadata_index\n",
        "        Args:\n",
        "            bq_dataset_id   :   Required (str) - BigQuery Dataset ID structured in the following format: \"google_project_name.dataset_name\"\n",
        "            bq_table_name   :   Required (str) - Desired BigQuery Table name\n",
        "            lb_dataset      :   Required (labelbox.schema.dataset.Dataset) - Labelbox dataset to add data rows to\n",
        "            metadata_index  :   Optional (dict) - Dictionary where {key=column_name : value=metadata_type} - metadata_type must be one of \"enum\", \"string\", \"datetime\" or \"number\"        \n",
        "        Returns:\n",
        "            If any, a list of errors from attempting to create BigQuery table rows from Labelbox data rows\n",
        "        \"\"\"\n",
        "        lb_mdo = self.lb_client.get_data_row_metadata_ontology()\n",
        "        # Create dictionary where {key = metadata_field_name : value = metadata_schema_id}\n",
        "        metadata_schema_to_name_key = self.__get_metadata_schema_to_name_key(lb_mdo, invert=False)\n",
        "        # Construct your BigQuery Table Schema with data_row_id and row_data columns\n",
        "        table_schema = [bigquery.SchemaField(\"data_row_id\", \"STRING\", mode=\"REQUIRED\"), bigquery.SchemaField(\"row_data\", \"STRING\", mode=\"REQUIRED\")]\n",
        "        # If data rows have external IDs, add an external_id column\n",
        "        data_row_export = list(lb_dataset.export_data_rows(include_metadata=True))\n",
        "        if data_row_export[0].external_id:\n",
        "            table_schema.append(bigquery.SchemaField(\"external_id\", \"STRING\"))\n",
        "        # If data rows have global_key IDs, add an global_key column            \n",
        "        if data_row_export[0].global_key:\n",
        "            table_schema.append(bigquery.SchemaField(\"global_key\", \"STRING\"))\n",
        "        if metadata_index:\n",
        "            # For each key in the metadata_index, make a column\n",
        "            for metadata_field_name in metadata_index.keys():\n",
        "                mdf = metadata_field_name.replace(\" \", \"_\")\n",
        "                table_schema.append(bigquery.SchemaField(mdf, \"STRING\"))\n",
        "        # Make your BigQuery table\n",
        "        bq_table_name = bq_table_name.replace(\"-\",\"_\") # BigQuery tables shouldn't have \"-\" in them, as this causes errors when performing SQL updates\n",
        "        bq_table = self.bq_client.create_table(bigquery.Table(f\"{bq_dataset_id}.{bq_table_name}\", schema=table_schema))\n",
        "        # Add your data rows as table rows\n",
        "        rows_to_insert = []\n",
        "        for lb_data_row in data_row_export:\n",
        "            row_dict = {\"data_row_id\" : lb_data_row.uid, \"row_data\" : lb_data_row.row_data}\n",
        "            if lb_data_row.external_id:\n",
        "                row_dict['external_id'] = lb_data_row.external_id\n",
        "            if lb_data_row.global_key:\n",
        "                row_dict['global_key'] = lb_data_row.global_key\n",
        "            if metadata_index:\n",
        "                field_to_value = {}\n",
        "                if lb_data_row.metadata_fields:\n",
        "                    for data_row_metadata in lb_data_row.metadata_fields:\n",
        "                        if data_row_metadata['value'] in metadata_schema_to_name_key.keys():\n",
        "                            field_to_value[data_row_metadata['name']] = metadata_schema_to_name_key[data_row_metadata['value']].split(\"///\")[1]\n",
        "                        else:\n",
        "                            field_to_value[data_row_metadata['name']] = data_row_metadata['value']\n",
        "                for metadata_field_name in metadata_index:\n",
        "                    mdf = metadata_field_name.replace(\" \", \"_\")\n",
        "                    if metadata_field_name in field_to_value.keys():\n",
        "                        row_dict[mdf] = field_to_value[metadata_field_name]\n",
        "            rows_to_insert.append(row_dict)\n",
        "        errors = self.bq_client.insert_rows_json(bq_table, rows_to_insert)\n",
        "        if not errors:\n",
        "            print(f'Success\\nCreated BigQuery Table with ID {bq_table.table_id}')\n",
        "        else:\n",
        "            print(errors)\n",
        "        return errors\n",
        "\n",
        "    def upsert_table_metadata(self, bq_table_id, lb_dataset, global_key_col, metadata_index={}):\n",
        "        \"\"\" Upserts a BigQuery Table based on the most recent metadata in Labelbox, only updates columns provided via a metadata_index keys\n",
        "        Args:\n",
        "            bq_table_id     :   Required (str) - BigQuery Table ID structured in the following format: \"google_project_name.dataset_name.table_name\"\n",
        "            lb_dataset      :   Required (labelbox.schema.dataset.Dataset) - Labelbox dataset to add data rows to\n",
        "            global_key_col  :   Required (str) - Global key column name to map Labelbox data rows to the BigQuery table rows\n",
        "            metadata_index  :   Optional (dict) - Dictionary where {key=column_name : value=metadata_type} - metadata_type must be one of \"enum\", \"string\", \"datetime\" or \"number\"        \n",
        "        Returns:\n",
        "            Updated BigQuery table\n",
        "        \"\"\"\n",
        "        # Sync metadata index keys with metadata ontology\n",
        "        check = self._sync_metadata_fields(bq_table_id, metadata_index)\n",
        "        if not check:\n",
        "          return None        \n",
        "        bq_table = self.bq_client.get_table(bq_table_id)\n",
        "        data_rows = lb_dataset.export_data_rows(include_metadata=True)\n",
        "        metadata_schema_to_name_key = self.__get_metadata_schema_to_name_key(self.lb_client.get_data_row_metadata_ontology(), invert=False)\n",
        "        # If a new metadata column needs to be made, make it\n",
        "        if metadata_index:\n",
        "            column_names = [schema_field.name.lower() for schema_field in bq_table.schema]\n",
        "            for metadata_field_name in metadata_index.keys():\n",
        "                mdf = metadata_field_name.lower().replace(\" \", \"_\")\n",
        "                if mdf not in column_names:\n",
        "                    new_schema = bq_table.schema[:]\n",
        "                    new_schema.append(bigquery.SchemaField(mdf, \"STRING\"))\n",
        "                    bq_table.schema = new_schema\n",
        "                    bq_table = self.bq_client.update_table(bq_table, [\"schema\"])\n",
        "        # Make one SQL update per data row\n",
        "        for data_row in data_rows:\n",
        "            query = False\n",
        "            query_str = f\"UPDATE {bq_table_id}\\nSET\"\n",
        "            field_to_value = {}\n",
        "            if data_row.metadata_fields:\n",
        "                for drm in data_row.metadata_fields:\n",
        "                    mdf = drm['name'].lower().replace(\" \", \"_\")\n",
        "                    field_to_value[mdf] = metadata_schema_to_name_key[drm['value']].split(\"///\")[1] if drm['value'] in metadata_schema_to_name_key.keys() else drm['value']\n",
        "            if metadata_index:\n",
        "                for metadata_field_name in metadata_index.keys():\n",
        "                    mdf = metadata_field_name.lower().replace(\" \", \"_\")\n",
        "                    if mdf in field_to_value.keys():\n",
        "                        query = True\n",
        "                        query_str += f'\\n   {mdf} = \"{field_to_value[mdf]}\",'     \n",
        "            query_str = query_str[:-1]                \n",
        "            if query:\n",
        "                query_str += f'\\nWHERE {global_key_col} = \"{data_row.global_key}\";'\n",
        "                query_job = self.bq_client.query(query_str)\n",
        "                query_job.result()\n",
        "        print(f'Success')\n",
        "\n",
        "    def upsert_labelbox_metadata(self, bq_table_id, global_key_col, global_keys_list=[], metadata_fields=[]):\n",
        "        \"\"\" Updates Labelbox data row metadata based on the most recent metadata from a Databricks spark table, only updates metadata fields provided via a metadata_index keys\n",
        "        Args:\n",
        "            bq_table_id         :   Required (str) - BigQuery Table ID structured in the following format: \"google_project_name.dataset_name.table_name\"\n",
        "            global_key_col      :   Required (str) - Global key column name to map Labelbox data rows to the BigQuery table rows\n",
        "            global_keys_list    :   Optional (list) - List of global keys you wish to upsert - defaults to the whole table\n",
        "            metadata_fields     :   Optional (dict) - List of column names to to upsert in the table\n",
        "        Returns:\n",
        "            List of errors from metadata ontology bulk upsert - if successful, is an empty list\n",
        "        \"\"\"\n",
        "        # Sync metadata index keys with metadata ontology\n",
        "        check = self._sync_metadata_fields(bq_table_id, metadata_index)\n",
        "        if not check:\n",
        "          return None        \n",
        "        lb_mdo = self.lb_client.get_data_row_metadata_ontology()\n",
        "        bq_table = self.bq_client.get_table(bq_table_id)\n",
        "        metadata_schema_to_name_key = self.__get_metadata_schema_to_name_key(lb_mdo, invert=False)\n",
        "        metadata_name_key_to_schema = self.__get_metadata_schema_to_name_key(lb_mdo, invert=True)        \n",
        "        # Create a query to pull global key and metadata from BigQuery\n",
        "        col_query = f\"\"\"{global_key_col}, \"\"\"\n",
        "        for metadata_field in metadata_fields:\n",
        "            col_query += metadata_field\n",
        "        query_str = f\"\"\"SELECT {col_query} FROM {bq_table_id}\"\"\"            \n",
        "        query_job = self.bq_client.query(query_str)\n",
        "        query_job.result()          \n",
        "        query_dict = {x[global_key_col] : x for x in query_job}\n",
        "        # Either use global_keys provided or all the global keys in the provided global_key_col\n",
        "        global_keys = global_keys_list if global_keys_list else list(query_dict.keys())\n",
        "        # Grab data row IDs with global_key list\n",
        "        data_row_ids = self.lb_client.get_data_row_ids_for_global_keys(global_keys)['results']\n",
        "        drid_to_global_key = {data_row_ids[i]: global_keys[i] for i in range(len(global_keys))}\n",
        "        # Get data row metadata with list of data row IDs\n",
        "        data_row_metadata = lb_mdo.bulk_export(data_row_ids)\n",
        "        upload_metadata = []  \n",
        "        for data_row in data_row_metadata:\n",
        "            drid = data_row.data_row_id\n",
        "            new_metadata = data_row.fields[:]\n",
        "            for field in new_metadata:\n",
        "                field_name = metadata_schema_to_name_key[field.schema_id]\n",
        "                if field_name in metadata_fields:\n",
        "                    ### Get the table value given a global key for each column in \n",
        "                    table_value = query_dict[drid_to_global_key[drid]][field_name]\n",
        "                    name_key = f\"{field_name}///{table_value}\"\n",
        "                    field.value = metadata_name_key_to_schema[name_key] if name_key in metadata_name_key_to_schema.keys() else table_value\n",
        "            upload_metadata.append(labelbox.schema.data_row_metadata.DataRowMetadata(data_row_id=drid, fields=new_metadata))\n",
        "        results = lb_mdo.bulk_upsert(upload_metadata)\n",
        "        return results        "
      ],
      "metadata": {
        "id": "LpQUx4aZnIGC"
      },
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up LabelBigQuery Client"
      ],
      "metadata": {
        "id": "mP-KdutcAoJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a service account key JSON file here: https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating"
      ],
      "metadata": {
        "id": "YvJZxMqH7G7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"\"\n",
        "google_project_name = \"\"\n",
        "path_to_service_account_key = \"\""
      ],
      "metadata": {
        "id": "UwGsiuVun-7V"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate a LabelBigQuery Client\n",
        "lbq_client = Client(\n",
        "    lb_api_key=api_key,\n",
        "    google_project_name=google_project_name,\n",
        "    google_key=path_to_service_account_key\n",
        ")"
      ],
      "metadata": {
        "id": "tsDJpOY3nJZe"
      },
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create a BigQuery Table from a Labelbox Dataset**\n",
        "\n",
        "`client.create_table_from_dataset()` will create a table from a Labelbox dataset given the following: \n",
        "\n",
        "- An existing `bq_dataset_id` (which is just `google_project_name.bq_dataset_name`)\n",
        "- A desired `bq_table_name`\n",
        "- An existing `labelbox_dataset_id` \n",
        "- An optional `metadata_index` \n",
        "  - This must be a dictionary where {key=metadata field name : value=metadata_type} \n",
        "    - `metadata_type` must be `\"string\"`, `\"enum\"`, `\"datetime\"` or `\"number\"`\n",
        "  - Each key passed in to the `metadata_index` will correspond to a column in BigQuery"
      ],
      "metadata": {
        "id": "cpq--1Yb8Nrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_project_name = \"\"\n",
        "bq_dataset_name = \"\"\n",
        "bq_dataset_id = f\"{google_project_name}.{bq_dataset_name}\"\n",
        "\n",
        "bq_table_name = \"\"\n",
        "\n",
        "labelbox_dataset_id = \"\"\n",
        "lb_dataset = lbq_client.lb_client.get_dataset(labelbox_dataset_id)\n",
        "\n",
        "metadata_index = {\n",
        "    \"\" : \"\"\n",
        "}"
      ],
      "metadata": {
        "id": "jD648-eY9PnD"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BigQuery Table from a Labelbox Dataset\n",
        "upload_results = lbq_client.create_table_from_dataset(\n",
        "    bq_dataset_id = bq_dataset_id,\n",
        "    bq_table_name = bq_table_name,\n",
        "    lb_dataset = lb_dataset,\n",
        "    metadata_index = metadata_index\n",
        ")"
      ],
      "metadata": {
        "id": "3JLN2bF_Veg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unhash to delete data_row_id column from BQ Table\n",
        "\n",
        "# bq_table_id = f\"{google_project_name}.{bq_dataset_name}.{bq_table_name}\"\n",
        "# del_col_query=f\"\"\"ALTER TABLE {bq_table_id}\n",
        "# DROP COLUMN data_row_id;\"\"\"\n",
        "# query_job = lbq_client.bq_client.query(del_col_query)\n",
        "# query_job.result()"
      ],
      "metadata": {
        "id": "QEeJr1fOFRes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Labelbox Data Rows from a BigQuery Table**\n",
        "\n",
        "`client.create_data_rows_from_table()` will create Labelbox data rows given the following:\n",
        "- An existing `bq_table_id` (which is just `google_project_name.bq_dataset_name.bq_table_name`)\n",
        "- An existing `lb_dataset` object\n",
        "- A column name for your Labelbox data row `row_data`\n",
        "- An optional column name for your Labelbox data row `global_key` - defaults to `row_data`\n",
        "- An optional column name for your Labelbox data row `external_id` - defaults to `global_key`\n",
        "- An optional `metadata_index` \n",
        "  - This must be a dictionary where {key=metadata field name : value=metadata_type} \n",
        "    - `metadata_type` must be `\"string\"`, `\"enum\"`, `\"datetime\"` or `\"number\"`\n",
        "  - Each key passed in to the `metadata_index` will correspond to a column in BigQuery\n",
        "- An optional `attachment_index` \n",
        "  - This must be a dictionary where {key=atachment field name : value=atachment_type} \n",
        "    - `attachment_type` must be `\"IMAGE\"`, `\"VIDEO\"`, `\"TEXT\"`, `\"HTML\"`\n",
        "  - Each key passed in to the `attachment_index` will correspond to a column in BigQuery  "
      ],
      "metadata": {
        "id": "qCoxeWHDgR8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unhash to create a dataset\n",
        "# lb_dataset = lbq_client.lb_client.create_dataset(name=bq_table_name)"
      ],
      "metadata": {
        "id": "GNhxFN-nFkO8"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_project_name = \"\"\n",
        "bq_dataset_name = \"\"\n",
        "bq_table_name = \"\"\n",
        "bq_table_id = f\"{google_project_name}.{bq_dataset_name}.{bq_table_name}\"\n",
        "\n",
        "labelbox_dataset_id = lb_dataset.uid\n",
        "lb_dataset = lbq_client.lb_client.get_dataset(labelbox_dataset_id)\n",
        "\n",
        "global_key_column = \"\"\n",
        "row_data_column = \"\"\n",
        "external_id_column = \"\"\n",
        "\n",
        "metadata_index = {\n",
        "    \"\" : \"\"\n",
        "}\n",
        "\n",
        "attachment_index = {\n",
        "    \"\" : \"\"\n",
        "}"
      ],
      "metadata": {
        "id": "9phCCdcS-z7y"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Labelbox Data Rows from a BigQuery Table\n",
        "upload_results = lbq_client.create_data_rows_from_table(\n",
        "    bq_table_id=bq_table_id,\n",
        "    row_data_col=row_data_column, \n",
        "    lb_dataset=lb_dataset, \n",
        "    global_key_col=global_key_column,\n",
        "    external_id_col=external_id_column, \n",
        "    metadata_index=metadata_index,\n",
        "    attachment_index=attachment_index,\n",
        "    skip_duplicates=True\n",
        ")"
      ],
      "metadata": {
        "id": "cd78P2vudpLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Upsert BigQuery Table Values with the Labelbox Metadata**"
      ],
      "metadata": {
        "id": "-2LzsXic898t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`client.upsert_table_metadata()` will upsert a BigQuery table based on most recent metadata from Labelbox given the following:\n",
        "- An existing `bq_table_id` (which is just `google_project_name.bq_dataset_name.bq_table_name`)\n",
        "- An existing `lb_dataset` object\n",
        "- A column name for your Labelbox data row `global_key`\n",
        "- A `metadata_index` where each key is a column you're looking to update in BigQuery\n",
        "  - This must be a dictionary where {key=metadata field name : value=metadata type} \n",
        "    - `metadata_type` must be `\"string\"`, `\"enum\"`, `\"datetime\"` or `\"number\"`\n",
        "  - Each key passed in to the `metadata index` will correspond to a column in BigQuery"
      ],
      "metadata": {
        "id": "tEzXtwN4hCNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_project_name = \"\"\n",
        "bq_dataset_name = \"\"\n",
        "bq_table_name = \"\"\n",
        "bq_table_id = f\"{google_project_name}.{bq_dataset_name}.{bq_table_name}\"\n",
        "\n",
        "labelbox_dataset_id = lb_dataset.uid\n",
        "lb_dataset = lbq_client.lb_client.get_dataset(labelbox_dataset_id)\n",
        "\n",
        "global_key_column = \"\"\n",
        "\n",
        "metadata_index = {\n",
        "    \"\" : \"\"\n",
        "}"
      ],
      "metadata": {
        "id": "oVx7xp9IAHvs"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = lbq_client.upsert_table_metadata(\n",
        "    bq_table_id = bq_table_id,\n",
        "    lb_dataset = lbq_client.lb_client.get_dataset(labelbox_dataset_id),\n",
        "    global_key_col = global_key_column,\n",
        "    metadata_index = metadata_index\n",
        ")"
      ],
      "metadata": {
        "id": "PS-OdYWONXBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Upsert Labelbox Metadata Values with BigQuery column data**\n",
        "\n",
        "client.upsert_labelbox_metadata() will upsert Labelbox metadata based on most recent column values from BigQuery given the following:\n",
        "\n",
        "- An existing `bq_table_id` (which is just `google_project_name.bq_dataset_name.bq_table_name`)\n",
        "- A list of `global_keys` that correspond to data rows to-be-upserted\n",
        "- A column name for your Labelbox data row `global_key`\n",
        "- A `metadata_index` where each key is a metadata field you're looking to upsert in Labelbox\n",
        "  - This must be a dictionary where {key=metadata field name : value=metadata type} \n",
        "    - `metadata_type` must be `\"string\"`, `\"enum\"`, `\"datetime\"` or `\"number\"`\n",
        "  - Each key passed in to the `metadata index` will correspond to a column in BigQuery"
      ],
      "metadata": {
        "id": "BYtdFPTz9Weq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_project_name = \"\"\n",
        "bq_dataset_name = \"\"\n",
        "bq_table_name = \"\"\n",
        "bq_table_id = f\"{google_project_name}.{bq_dataset_name}.{bq_table_name}\"\n",
        "\n",
        "global_keys_list = []\n",
        "\n",
        "global_key_column = \"\"\n",
        "\n",
        "metadata_index = {\n",
        "    \"\" : \"\"\n",
        "}\n",
        "metadata_fields = list(metadata_index.keys())"
      ],
      "metadata": {
        "id": "O4vTcCgN9_ao"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = lbq_client.upsert_labelbox_metadata(\n",
        "    bq_table_id = bq_table_id,\n",
        "    global_key_col = global_key_column,\n",
        "    global_keys_list = global_keys_list, # Will take all global keys from table if False\n",
        "    metadata_fields = metadata_fields\n",
        ")"
      ],
      "metadata": {
        "id": "lpo_WuC4-DrF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}